{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f746621",
   "metadata": {},
   "source": [
    "## RAG with MongoDB - \n",
    "\n",
    "### 1. Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d729324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca2df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import  OpenAI\n",
    "\n",
    "## initialize the client\n",
    "client = OpenAI()\n",
    "\n",
    "## specify the embedding model\n",
    "model = 'text-embedding-3-large' \n",
    "\n",
    "## Define the function to generate embedding\n",
    "\n",
    "def get_embedding(text, input_type=\"document\"):\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd783d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_embedding(\"RAG technology\")\n",
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0053af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data ingestion\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1fe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the pdf\n",
    "loader = PyPDFLoader(\"https://investors.mongodb.com/node/12236/pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "## split the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d899382",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare documents for insertion\n",
    "docs_to_insert = [\n",
    "    {\n",
    "        \"text\": doc.page_content,\n",
    "        \"embedding\": get_embedding(doc.page_content)\n",
    "    } for doc in documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c57dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "mongo_user = os.getenv(\"MONGO_USER\")\n",
    "mongo_pass = os.getenv(\"MONGO_PASS\")\n",
    "mongo_host = os.getenv(\"MONGO_HOST\")\n",
    "mongo_db = os.getenv(\"MONGO_DB\", \"admin\")\n",
    "\n",
    "## connect to your MongoDB  Deployment\n",
    "client = MongoClient(f\"mongodb+srv://{mongo_user}:{mongo_pass}@{mongo_host}/?appName=Cluster0\")\n",
    "collection = client[\"sample_mflix\"][\"rag_pdf\"]\n",
    "\n",
    "## insert documents into the collection\n",
    "result = collection.insert_many(docs_to_insert)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c7f83",
   "metadata": {},
   "source": [
    "### 2. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987c4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### query with search index\n",
    "from pymongo.operations import SearchIndexModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b9103fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vector_index'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create your index model, then create the ssearch index\n",
    "index_name = \"vector_index\"\n",
    "search_index_model = SearchIndexModel(\n",
    "    definition = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"numDimensions\": 3072,\n",
    "                \"path\": \"embedding\",\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    name = index_name,\n",
    "    type = \"vectorSearch\"\n",
    ")\n",
    "\n",
    "collection.create_search_index(model=search_index_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be5391b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polling to check if the inndex is ready. This may take up to a minute\n",
      "vector_index is ready for querying\n"
     ]
    }
   ],
   "source": [
    "# wait for initial sync to complete\n",
    "print(\"Polling to check if the inndex is ready. This may take up to a minute\")\n",
    "predicate = None\n",
    "if predicate is None:\n",
    "    predicate = lambda index: index.get(\"queryable\") is True\n",
    "\n",
    "while True:\n",
    "    indices = list(collection.list_search_indexes(index_name))\n",
    "    if len(indices) and predicate(indices[0]):\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "print(index_name + \" is ready for querying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca46e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = get_embedding(\"AI Technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd6d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.rag_pdf.aggregate(\n",
    "    [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": 3072,\n",
    "                \"limit\": 5,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ce2d0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_of_results = []\n",
    "for doc in results:\n",
    "    array_of_results.append(doc)\n",
    "\n",
    "array_of_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a40faa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run vector search queries\n",
    "def get_query_results(query):\n",
    "    \"\"\" Gets results from a vector search query \"\"\"\n",
    "\n",
    "    query_embedding = get_embedding(query, input_type=\"query\")\n",
    "    # print(query_embedding)\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 3072,\n",
    "                \"limit\": 5,\n",
    "            }\n",
    "        }, {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"text\": 1,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = collection.aggregate(pipeline)\n",
    "    # print(results)\n",
    "\n",
    "    array_of_results = []\n",
    "    for doc in results:\n",
    "        array_of_results.append(doc)    \n",
    "        \n",
    "    return array_of_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf009d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test the funcion with a sample query // print len of results\n",
    "len(get_query_results(\"mongodb vector search\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070ec97",
   "metadata": {},
   "source": [
    "### 3. Generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbcd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MongoDB's latest AI announcements include the launch of the MongoDB AI Applications Program (MAAP), which aims to expand their AI ecosystem by providing customers with reference architectures, pre-built partner integrations, and professional services to help them quickly build AI-powered applications. Additionally, they have announced MongoDB 8.0, which features significant performance improvements like faster reads, updates, bulk inserts, and time series queries. They have also made Atlas Stream Processing generally available, enabling customers to build sophisticated, event-driven applications with real-time data. Furthermore, Accenture will establish a center of excellence focused on MongoDB projects and is the first global systems integrator to join MAAP.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Specify search query, retrieve relevat documents, and convert to string\n",
    "query = \"What are MongoDB's latest AI announcements?\"\n",
    "context_docs = get_query_results(query)\n",
    "context_string = \" \".join([doc[\"text\"] for doc in context_docs])\n",
    "\n",
    "# Construct prompt for the LLM using the retrieved documents as the context\n",
    "prompt = f\"\"\"\n",
    "Use the following context to answer the question at the end.\n",
    "Context: {context_string}\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# OpenAI model to use\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "# \"MongoDB's latest AI announcements include the launch of the MongoDB ....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a392833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e6b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
